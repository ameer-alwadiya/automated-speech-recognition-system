{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3fba9f",
   "metadata": {},
   "source": [
    "# SegNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1da5ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Activation\n",
    "from tensorflow.python.layers.normalization import BatchNormalization\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from PIL import Image\n",
    "\n",
    "def segnet(input_shape=(256, 256, 3), classes=1):\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(img_input)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = ELU()(conv1)\n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = ELU()(conv1)\n",
    "    pool1 = MaxPooling2D()(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = ELU()(conv2)\n",
    "    conv2 = Conv2D(128, (3, 3), padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = ELU()(conv2)\n",
    "    pool2 = MaxPooling2D()(conv2)\n",
    "\n",
    "    # Decoder\n",
    "    up1 = UpSampling2D()(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), padding='same')(up1)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = ELU()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = ELU()(conv3)\n",
    "\n",
    "    up2 = UpSampling2D()(conv3)\n",
    "    conv4 = Conv2D(64, (3, 3), padding='same')(up2)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = ELU()(conv4)\n",
    "    conv4 = Conv2D(64, (3, 3), padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = ELU()(conv4)\n",
    "\n",
    "    # Output layer\n",
    "    out = Conv2D(classes, (1, 1), activation='sigmoid', padding='same')(conv4)\n",
    "    out = Reshape((input_shape[0] * input_shape[1], classes))(out)\n",
    "\n",
    "    model = Model(inputs=img_input, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65cd72d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The following are legacy tf.layers.Layers:\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x169801c10>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x16a23f410>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x169079ed0>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x1698f8250>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x16a1c8090>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x1697c1c50>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x1698f7750>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x16a23d8d0>\nTo use keras as a framework (for instance using the Network, Model, or Sequential classes), please use the tf.keras.layers implementation instead. (Or, if writing custom layers, subclass from tf.keras.layers rather than tf.layers)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      9\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Binary segmentation, change to the number of classes for multi-class segmentation\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m segnet(input_shape, classes)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegnet_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load pre-trained weights if available\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Perform segmentation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 49\u001b[0m, in \u001b[0;36msegnet\u001b[0;34m(input_shape, classes)\u001b[0m\n\u001b[1;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m Conv2D(classes, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(conv4)\n\u001b[1;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m Reshape((input_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m input_shape[\u001b[38;5;241m1\u001b[39m], classes))(out)\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mimg_input, outputs\u001b[38;5;241m=\u001b[39mout)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/engine/functional.py:167\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    159\u001b[0m         [\n\u001b[1;32m    160\u001b[0m             functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[1;32m    162\u001b[0m         ]\n\u001b[1;32m    163\u001b[0m     ):\n\u001b[1;32m    164\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(\n\u001b[1;32m    165\u001b[0m             inputs, outputs\n\u001b[1;32m    166\u001b[0m         )\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_graph_network(inputs, outputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/engine/functional.py:299\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_tensor_usage_count()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_save_spec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nested_inputs)\n\u001b[0;32m--> 299\u001b[0m tf_utils\u001b[38;5;241m.\u001b[39massert_no_legacy_layers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Note that this method is used by both functional and sequential\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# models, so we can't just have this method in functional.__init__,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# which will miss the coverage of sequential model.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/utils/tf_utils.py:555\u001b[0m, in \u001b[0;36massert_no_legacy_layers\u001b[0;34m(layers)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m legacy_layers:\n\u001b[1;32m    554\u001b[0m     layer_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m legacy_layers)\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following are legacy tf.layers.Layers:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlayer_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use keras as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework (for instance using the Network, Model, or Sequential \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses), please use the tf.keras.layers implementation instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Or, if writing custom layers, subclass from tf.keras.layers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrather than tf.layers)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: The following are legacy tf.layers.Layers:\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x169801c10>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x16a23f410>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x169079ed0>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x1698f8250>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x16a1c8090>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x1697c1c50>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x1698f7750>\n  <keras.legacy_tf_layers.normalization.BatchNormalization object at 0x16a23d8d0>\nTo use keras as a framework (for instance using the Network, Model, or Sequential classes), please use the tf.keras.layers implementation instead. (Or, if writing custom layers, subclass from tf.keras.layers rather than tf.layers)"
     ]
    }
   ],
   "source": [
    "# Load and preprocess image data\n",
    "img = Image.open('mexico.png')\n",
    "img = img.resize((256, 256))  # Resize the image if needed\n",
    "img_arr = np.array(img)\n",
    "img_arr = img_arr / 255.0  # Normalize pixel values\n",
    "\n",
    "# Apply SegNet\n",
    "input_shape = (256, 256, 3)\n",
    "classes = 1  # Binary segmentation, change to the number of classes for multi-class segmentation\n",
    "model = segnet(input_shape, classes)\n",
    "model.load_weights('segnet_weights.h5')  # Load pre-trained weights if available\n",
    "\n",
    "# Perform segmentation\n",
    "segmentation = model.predict(np.expand_dims(img_arr, axis=0))\n",
    "\n",
    "# Post-process segmentation if needed (e.g., thresholding, color mapping)\n",
    "\n",
    "# Output segmented image or use for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48efd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
