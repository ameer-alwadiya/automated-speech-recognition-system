{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: YOU SHOULD ONLY NEED TO RUN THIS STEP THE FIRST TIME IN A SESSION\n",
    "# get the data from github and unzip\n",
    "# The term \"wget\" stands for \"World Wide Web get.\" It's a command-line utility for downloading files from the internet.\n",
    "!curl -O https://raw.githubusercontent.com/andrsn/data/main/speechImageData.zip\n",
    "!unzip -q speechImageData.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f3271",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209837d2",
   "metadata": {},
   "source": [
    "# 1.1 Orignal data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efd389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='speechImageData - Copy/TrainData',\n",
    "    labels='inferred',\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=128,\n",
    "    image_size=(98, 50)\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='speechImageData - Copy/ValData',\n",
    "    labels='inferred',\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=128,\n",
    "    image_size=(98, 50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract the  training input images and output class labels\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "\"\"\"The take method is used to create a new dataset containing a specified number of elements from the original dataset.\n",
    "When -1 is provided as the argument, it essentially means \"take all elements\" from the original dataset.\"\"\"\n",
    "for images, labels in train_ds.take(-1):\n",
    "    x_train.append(images.numpy())\n",
    "    y_train.append(labels.numpy())\n",
    "\n",
    "# axis=0 means the arrays will be concatenated along the first axis (i.e., rows will be stacked vertically).\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "# Extract the validation input images and output class labels\n",
    "x_val = []\n",
    "y_val = []\n",
    "for images, labels in val_ds.take(-1):\n",
    "    x_val.append(images.numpy())\n",
    "    y_val.append(labels.numpy())\n",
    "\n",
    "x_val = np.concatenate(x_val, axis=0)\n",
    "y_val = np.concatenate(y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the feature training data: ', x_train.shape)\n",
    "print('Shape of the label training data: ', y_train.shape)\n",
    "print('--------------------------------------------------')\n",
    "print('Shape of an example of the feature training data: ', x_train[0].shape)\n",
    "print('Shape of an example of the label training data: ', y_train[0].shape)\n",
    "print('--------------------------------------------------')\n",
    "print('Shape of the feature testing data: ', x_val.shape)\n",
    "print('Shape of the label testing data: ', y_val.shape)\n",
    "print('--------------------------------------------------')\n",
    "# Get the class names (labels)\n",
    "label_names = train_ds.class_names\n",
    "print('Label names: ', label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Plot some examples\n",
    "num_examples = 5\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    plt.subplot(1, num_examples, i + 1)\n",
    "    rand_index = np.random.randint(0, x_train.shape[0])\n",
    "    plt.imshow(x_train[rand_index], plt.cm.binary)\n",
    "    plt.title(f\"Label: {label_names[np.argmax(y_train[rand_index])]}\") # np.argmax() returns the index of the maximum value in the array.\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a0ebf",
   "metadata": {},
   "source": [
    "# 1.2 Spectrogram Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09822",
   "metadata": {},
   "source": [
    "# 1.2.1 Frequency and time masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def spec_augment(spec, num_freq_masks=1, num_time_masks=1, freq_masking_max_percentage=0.1, time_masking_max_percentage=0.1):\n",
    "    spec_aug = spec.copy()\n",
    "    max_percentage_freq = int(freq_masking_max_percentage * spec.shape[0])\n",
    "    max_percentage_time = int(time_masking_max_percentage * spec.shape[1])\n",
    "    \n",
    "    for _ in range(num_freq_masks):\n",
    "        f = np.random.randint(0, max_percentage_freq)\n",
    "        f0 = np.random.randint(0, spec.shape[0] - f)\n",
    "        spec_aug[f0:f0 + f, :] = 0\n",
    "    \n",
    "    for _ in range(num_time_masks):\n",
    "        t = np.random.randint(0, max_percentage_time)\n",
    "        t0 = np.random.randint(0, spec.shape[1] - t)\n",
    "        spec_aug[:, t0:t0 + t] = 0\n",
    "    \n",
    "    return spec_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented = np.array([spec_augment(spec) for spec in x_train])\n",
    "\n",
    "print(x_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2897ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined1 = np.concatenate([x_train, x_train_augmented], axis=0)\n",
    "y_train_combined1 = np.concatenate([y_train, y_train], axis=0)\n",
    "\n",
    "print(x_train_combined1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original spectrogram\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Spectrogram')\n",
    "plt.imshow(x_train[6], plt.cm.binary)\n",
    "\n",
    "\n",
    "# Display the augmented spectrogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Augmented Spectrogram')\n",
    "plt.imshow(x_train_augmented[6], plt.cm.binary)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a34e1",
   "metadata": {},
   "source": [
    "# 1.2.2 Spectograms mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(original_melspecs, original_labels, alpha=0.5):\n",
    "    indices = np.random.permutation(original_melspecs.shape[0])\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "    augmented_melspecs = original_melspecs * lam + original_melspecs[indices] * (1 - lam)\n",
    "    augmented_labels = original_labels * lam + original_labels[indices] * (1 - lam)\n",
    "\n",
    "    return augmented_melspecs, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_x_train, augmented_y_train = mixup(x_train, y_train)\n",
    "print(augmented_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a148533",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined2 = np.concatenate([x_train_combined1, augmented_x_train], axis=0)\n",
    "y_train_combined2 = np.concatenate([y_train_combined1, augmented_y_train], axis=0)\n",
    "\n",
    "print(x_train_combined2.shape)\n",
    "print(y_train_combined2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b20930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original spectrogram\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Spectrogram')\n",
    "plt.imshow(x_train[3], plt.cm.binary)\n",
    "\n",
    "\n",
    "# Display the augmented spectrogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Augmented Spectrogram')\n",
    "plt.imshow(augmented_x_train[3], plt.cm.binary)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0861b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined3 = np.concatenate([x_train, augmented_x_train], axis=0)\n",
    "y_train_combined3 = np.concatenate([y_train, augmented_y_train], axis=0)\n",
    "\n",
    "print(x_train_combined3.shape)\n",
    "print(y_train_combined3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210323ee",
   "metadata": {},
   "source": [
    "# 2 Main Model (Basic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dcb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Softmax, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2  # Import l2 regularizer\n",
    "\n",
    "def create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0.25, learning_rate=0.001, weight_decay=0.001):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters[0], kernel_size=(3, 3), padding='same', input_shape=(98, 50, 1), kernel_regularizer=l2(weight_decay)))  # Add L2 to kernel\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    for i in range(num_layers-1):\n",
    "        model.add(Conv2D(filters[1], kernel_size=(3, 3), padding='same', kernel_regularizer=l2(weight_decay)))  # Add L2 to all convolutional layers\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Conv2D(filters[2], kernel_size=(3, 3), padding='same', kernel_regularizer=l2(weight_decay)))  # Add L2 to the last convolutional layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding = 'same'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(12))  # No regularization typically applied to Dense layers\n",
    "    model.add(Softmax())\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3558a1",
   "metadata": {},
   "source": [
    "# 2.1 Training using x_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58523ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
    "\n",
    "# Train the model\n",
    "model_1.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_1.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b9c5f",
   "metadata": {},
   "source": [
    "# 2.2 Training using x_train_combined1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
    "\n",
    "# Train the model\n",
    "model_2.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_2.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72cb39",
   "metadata": {},
   "source": [
    "# 2.3 Training using x_train_combined2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680538e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
    "\n",
    "# Train the model\n",
    "model_3.fit(x_train_combined2, y_train_combined2, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_3.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c74a3",
   "metadata": {},
   "source": [
    "# 2.4 Training using x_train_combined3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aa158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
    "\n",
    "# Train the model\n",
    "model_4.fit(x_train_combined3, y_train_combined3, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_4.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693c104",
   "metadata": {},
   "source": [
    "- We can observe that model_2 produces the best results when trained with the combined x_train data, along with frequency and time masking, yielding an accuracy of 0.662. This suggests that combining spectogram data with x_train data, and frequency and time masking, may not be effective, as it leads to slightly less accurate results, around 0.625. Combining x_train with spectogram-mixed data might not be advisable, as evidenced by the accuracy outcome of 0.607 for x_train_combined3. This indicates that mixing spectogram data when combined results in lower accuracy. Therefore, we may exclude them from our upcoming training and testing processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984dfdf0",
   "metadata": {},
   "source": [
    "# 3 Grid Search Optimizatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf811f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5,\n",
    "                        verbose=1, num_layers=2,\n",
    "                        filters=(8, 16, 32))\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {\n",
    "    'num_layers': [2, 3, 4, 6, 8],\n",
    "    'filters': [(8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(x_train, y_train, batch_size=32, epochs=15, validation_split=0.3)\n",
    "\n",
    "# Print results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca03f1",
   "metadata": {},
   "source": [
    "- The results of Grid Search Optimizatiom have been copied from Google Colab with the same configurations, as it took for ever using my processor and Jupyter notebook to implement.\n",
    "- This is the last page of the training process since it contains a huge number of pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e1f13",
   "metadata": {},
   "source": [
    "Epoch 1/15\n",
    "44/44 [==============================] - 5s 39ms/step - loss: 3.1232 - accuracy: 0.2807 - val_loss: 10.5849 - val_accuracy: 0.1614\n",
    "Epoch 2/15\n",
    "44/44 [==============================] - 1s 19ms/step - loss: 2.1057 - accuracy: 0.4607 - val_loss: 2.8025 - val_accuracy: 0.3760\n",
    "Epoch 3/15\n",
    "44/44 [==============================] - 1s 19ms/step - loss: 1.4567 - accuracy: 0.6086 - val_loss: 1.6601 - val_accuracy: 0.5724\n",
    "Epoch 4/15\n",
    "44/44 [==============================] - 1s 20ms/step - loss: 1.0866 - accuracy: 0.7386 - val_loss: 2.1801 - val_accuracy: 0.4659\n",
    "Epoch 5/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.9935 - accuracy: 0.7636 - val_loss: 1.1725 - val_accuracy: 0.7255\n",
    "Epoch 6/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.8360 - accuracy: 0.8107 - val_loss: 1.2803 - val_accuracy: 0.7238\n",
    "Epoch 7/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.6631 - accuracy: 0.8686 - val_loss: 0.9321 - val_accuracy: 0.7870\n",
    "Epoch 8/15\n",
    "44/44 [==============================] - 1s 16ms/step - loss: 0.5331 - accuracy: 0.9164 - val_loss: 0.9160 - val_accuracy: 0.8103\n",
    "Epoch 9/15\n",
    "44/44 [==============================] - 1s 16ms/step - loss: 0.4642 - accuracy: 0.9364 - val_loss: 1.1498 - val_accuracy: 0.7304\n",
    "Epoch 10/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.4359 - accuracy: 0.9429 - val_loss: 1.0978 - val_accuracy: 0.7704\n",
    "Epoch 11/15\n",
    "44/44 [==============================] - 1s 15ms/step - loss: 0.3780 - accuracy: 0.9693 - val_loss: 0.8341 - val_accuracy: 0.8469\n",
    "Epoch 12/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.3812 - accuracy: 0.9629 - val_loss: 1.2758 - val_accuracy: 0.7720\n",
    "Epoch 13/15\n",
    "44/44 [==============================] - 1s 16ms/step - loss: 0.3907 - accuracy: 0.9614 - val_loss: 1.0148 - val_accuracy: 0.8136\n",
    "Epoch 14/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.4230 - accuracy: 0.9471 - val_loss: 0.9516 - val_accuracy: 0.8087\n",
    "Epoch 15/15\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.3364 - accuracy: 0.9786 - val_loss: 1.0595 - val_accuracy: 0.8203\n",
    "\n",
    "Best: 0.747126 using {'filters': (32, 64, 128), 'num_layers': 4}\n",
    "\n",
    "0.635182 (0.014959) with: {'filters': (8, 16, 32), 'num_layers': 2}\n",
    "0.657671 (0.021634) with: {'filters': (8, 16, 32), 'num_layers': 3}\n",
    "0.614693 (0.037351) with: {'filters': (8, 16, 32), 'num_layers': 4}\n",
    "0.462269 (0.028821) with: {'filters': (8, 16, 32), 'num_layers': 6}\n",
    "0.405297 (0.051825) with: {'filters': (8, 16, 32), 'num_layers': 8}\n",
    "0.646177 (0.057168) with: {'filters': (16, 32, 64), 'num_layers': 2}\n",
    "0.690155 (0.053880) with: {'filters': (16, 32, 64), 'num_layers': 3}\n",
    "0.677161 (0.034327) with: {'filters': (16, 32, 64), 'num_layers': 4}\n",
    "0.591704 (0.036751) with: {'filters': (16, 32, 64), 'num_layers': 6}\n",
    "0.525737 (0.032023) with: {'filters': (16, 32, 64), 'num_layers': 8}\n",
    "0.631184 (0.033323) with: {'filters': (32, 64, 128), 'num_layers': 2}\n",
    "0.714143 (0.055860) with: {'filters': (32, 64, 128), 'num_layers': 3}\n",
    "0.747126 (0.051535) with: {'filters': (32, 64, 128), 'num_layers': 4}\n",
    "0.619190 (0.046501) with: {'filters': (32, 64, 128), 'num_layers': 6}\n",
    "0.502249 (0.078507) with: {'filters': (32, 64, 128), 'num_layers': 8}\n",
    "0.597201 (0.052827) with: {'filters': (64, 128, 256), 'num_layers': 2}\n",
    "0.655672 (0.063399) with: {'filters': (64, 128, 256), 'num_layers': 3}\n",
    "0.682159 (0.120408) with: {'filters': (64, 128, 256), 'num_layers': 4}\n",
    "0.704648 (0.029910) with: {'filters': (64, 128, 256), 'num_layers': 6}\n",
    "0.432284 (0.159571) with: {'filters': (64, 128, 256), 'num_layers': 8}\n",
    "0.562219 (0.072255) with: {'filters': (128, 256, 512), 'num_layers': 2}\n",
    "0.731634 (0.036231) with: {'filters': (128, 256, 512), 'num_layers': 3}\n",
    "0.636182 (0.067387) with: {'filters': (128, 256, 512), 'num_layers': 4}\n",
    "0.566717 (0.018033) with: {'filters': (128, 256, 512), 'num_layers': 6}\n",
    "0.570215 (0.050194) with: {'filters': (128, 256, 512), 'num_layers': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125d808",
   "metadata": {},
   "source": [
    "# 3.1 Training using x_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_1 = create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0.25, weight_decay=0.001)\n",
    "\n",
    "# Train the model\n",
    "grid_model_1.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = grid_model_1.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0451c7",
   "metadata": {},
   "source": [
    "# 3.2 Training using x_train_combined1 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fde171",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_2 = create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0.25, weight_decay=0.001)\n",
    "\n",
    "# Train the model\n",
    "grid_model_2.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = grid_model_2.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52aba2",
   "metadata": {},
   "source": [
    "- Both models grid_model_1 (Test Accuracy: 0.568) and grid_model_2 (Test Accuracy: 0.690) produce higher accuracies than the previous basic models model_1 (Test Accuracy: 0.547) and model_2 (Test Accuracy: 0.662)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6ee85",
   "metadata": {},
   "source": [
    "# 4 Bayesian Optimization (Tree-structured Parzen Estimator TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac78b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp\n",
    "# fmin: Stands for \"Function Minimization\".\n",
    "# tpe: Stands for \"Tree-structured Parzen Estimator\".\n",
    "# hp: Stands for \"Hyperparameters\".\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'num_layers': hp.choice('num_layers', [2, 3, 4, 6, 8]),\n",
    "    'filters': hp.choice('filters', [(8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)]),\n",
    "\n",
    "    'dropout_rate': hp.choice('dropout_rate', [0.0, 0.1, 0.15, 0.2, 0.25]),\n",
    "    'weight_decay': hp.choice('weight_decay', [0.0, 0.1, 0.01, 0.001, 0.0001]),\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = create_model(**params)  # Create model with current parameters\n",
    "\n",
    "    # Train the model and evaluate on the test set\n",
    "    model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
    "    loss, accuracy = model.evaluate(x_val, y_val)\n",
    "    return {'loss': -accuracy, 'status': 'ok'}\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66856c",
   "metadata": {},
   "source": [
    "- This is the last page of the training process since it contains a huge number of pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ba124",
   "metadata": {},
   "source": [
    " 1/37 ━━━━━━━━━━━━━━━━━━━━ 22s 630ms/step - accuracy: 0.4688 - loss: 2.1764\n",
    " 2/37 ━━━━━━━━━━━━━━━━━━━━ 4s 142ms/step - accuracy: 0.4609 - loss: 2.1940 \n",
    " 3/37 ━━━━━━━━━━━━━━━━━━━━ 4s 145ms/step - accuracy: 0.4497 - loss: 2.1858\n",
    " 4/37 ━━━━━━━━━━━━━━━━━━━━ 3s 117ms/step - accuracy: 0.4505 - loss: 2.1654\n",
    " 5/37 ━━━━━━━━━━━━━━━━━━━━ 4s 125ms/step - accuracy: 0.4567 - loss: 2.1245\n",
    " 6/37 ━━━━━━━━━━━━━━━━━━━━ 3s 113ms/step - accuracy: 0.4595 - loss: 2.0960\n",
    " 8/37 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - accuracy: 0.4649 - loss: 2.0637 \n",
    " 9/37 ━━━━━━━━━━━━━━━━━━━━ 2s 91ms/step - accuracy: 0.4657 - loss: 2.0619\n",
    "11/37 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - accuracy: 0.4671 - loss: 2.0646\n",
    "12/37 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - accuracy: 0.4683 - loss: 2.0611\n",
    "13/37 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - accuracy: 0.4691 - loss: 2.0600\n",
    "14/37 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - accuracy: 0.4692 - loss: 2.0587\n",
    "15/37 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - accuracy: 0.4689 - loss: 2.0605\n",
    "17/37 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - accuracy: 0.4685 - loss: 2.0631\n",
    "18/37 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - accuracy: 0.4682 - loss: 2.0634\n",
    "19/37 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - accuracy: 0.4680 - loss: 2.0621\n",
    "21/37 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - accuracy: 0.4672 - loss: 2.0616\n",
    "23/37 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - accuracy: 0.4668 - loss: 2.0613\n",
    "25/37 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - accuracy: 0.4664 - loss: 2.0603\n",
    "27/37 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - accuracy: 0.4660 - loss: 2.0599\n",
    "29/37 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - accuracy: 0.4657 - loss: 2.0615\n",
    "31/37 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - accuracy: 0.4657 - loss: 2.0615\n",
    "33/37 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - accuracy: 0.4661 - loss: 2.0596\n",
    "35/37 ━━━━━━━━━━━━━━━━━━━━ 0s 62ms/step - accuracy: 0.4661 - loss: 2.0589\n",
    "36/37 ━━━━━━━━━━━━━━━━━━━━ 0s 62ms/step - accuracy: 0.4662 - loss: 2.0583\n",
    "37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - accuracy: 0.4664 - loss: 2.0573\n",
    "37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 79ms/step - accuracy: 0.4665 - loss: 2.0564\n",
    "\n",
    "100%|████████| 20/20 [32:15<00:00, 96.77s/trial, best loss: -0.6379163265228271]\n",
    "\n",
    "Best parameters found:\n",
    "{'dropout_rate': 3, 'filters': 2, 'num_layers': 1, 'weight_decay': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a38856",
   "metadata": {},
   "source": [
    "# 4.1 Training using x_train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_model_1 = create_model(num_layers=3, filters=(32, 64, 128), dropout_rate=0.2, weight_decay=0.001)\n",
    "\n",
    "# Train the model\n",
    "bayesian_model_1.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = bayesian_model_1.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0796da0",
   "metadata": {},
   "source": [
    "# 4.2 Training using x_train_combined1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c31acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_model_2 = create_model(num_layers=3, filters=(32, 64, 128), dropout_rate=0.2, weight_decay=0.001)\n",
    "\n",
    "# Train the model\n",
    "bayesian_model_2.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = bayesian_model_2.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5bd3a",
   "metadata": {},
   "source": [
    "- The first model bayesian_model_1 (Test Accuracy: 0.593) produce higher accuracy than grid_model_1 (Test Accuracy: 0.568) and bayesian_model_2 (Test Accuracy: 0.722) produce higher accuracy than the previous model  grid_model_2 (Test Accuracy: 0.690)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dba3a",
   "metadata": {},
   "source": [
    "# 5 Monitoring Training Progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = bayesian_model_2.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=25, validation_split=0.5)\n",
    "\n",
    "# Access training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_accuracy, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e637ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = bayesian_model_2.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c8218",
   "metadata": {},
   "source": [
    "- Training the model with the number of epochs=25, resulted in a better accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60803cbf",
   "metadata": {},
   "source": [
    "# 5.1 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab51e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain model predictions and convert softmax outputs 0-1 to integer class label predictions\n",
    "Yhat = bayesian_model_2.predict(x_val)                 # predict model outputs on validation data as softmax outputs of probability of each class\n",
    "Yhat_integer = np.argmax(Yhat, axis=1)      # obtain the most likely class prediction as the argument of the max softmax output\n",
    "Y_test_integer = np.argmax(y_val, axis=1)   # obtain the true class as an integer\n",
    "\n",
    "# calculate and plot confusion matrix\n",
    "cm = confusion_matrix(Y_test_integer, Yhat_integer , normalize=\"pred\")    # calculate the confusion matrix\n",
    "plt.figure(figsize=(10, 8))  # setup new figure\n",
    "sns.heatmap(cm/np.sum(cm), annot=True, fmt=\".2%\", cmap=\"Blues\",)          # plot the confusion matrix using the sns package\n",
    "plt.title(\"Confusion Matrix\", fontsize = 12)                              # title\n",
    "plt.xlabel(\"Predicted Class\", fontsize = 12)                              # xlabel\n",
    "plt.ylabel(\"True Class\", fontsize = 12)                                   # ylabel\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb2df3",
   "metadata": {},
   "source": [
    "# 6 Model Averging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6260da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(x_train, y_train):\n",
    "    # Define number of samples\n",
    "    nsamples = int(x_train.shape[0]/2)\n",
    "\n",
    "    # Create data index\n",
    "    data_index = list(range(1, nsamples))\n",
    "\n",
    "    # Create random index using sampling with replacement\n",
    "    idx = random.choices(data_index, k=nsamples)\n",
    "\n",
    "    # Initialize data set 1\n",
    "    x1 = np.zeros([nsamples, 98, 50, 1])\n",
    "    y1 = np.zeros([nsamples, 12])\n",
    "\n",
    "    # Resample training data with replacement\n",
    "    for i in range(nsamples):\n",
    "        x1[i] = x_train[idx[i], :, :, :]\n",
    "        y1[i] = y_train[idx[i], :]\n",
    "\n",
    "    return x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 3\n",
    "\n",
    "# Initialize list to store individual model predictions\n",
    "predictions = []\n",
    "\n",
    "# Train and make predictions for each model\n",
    "for _ in range(num_models):\n",
    "    # Resample the data\n",
    "    x1, y1 = resample_data(x_train_combined1, y_train_combined1)\n",
    "    print(x1.shape)\n",
    "    # Initialize and train a model\n",
    "    model = create_model(num_layers=3, filters=(32, 64, 128), dropout_rate=0.2, weight_decay=0.001)\n",
    "    model.fit(x1, y1, batch_size=64, epochs=25, validation_split=0.5)\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(x_val, y_val)\n",
    "    predictions.append(test_accuracy)  \n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    \n",
    "# Calculate average prediction across all models\n",
    "average_prediction = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12678c1d",
   "metadata": {},
   "source": [
    "# 7 Multi-Branch Networks (Inception)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745abae",
   "metadata": {},
   "source": [
    "![](https://d2l.ai/_images/inception.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(tf.keras.Model):\n",
    "    def __init__(self, c1, c2, c3, c4):\n",
    "        super().__init__()\n",
    "        self.b1_1 = Conv2D(c1, 1, activation='relu')\n",
    "        \n",
    "        self.b2_1 = Conv2D(c2[0], 1, activation='relu')\n",
    "        self.b2_2 = Conv2D(c2[1], 3, padding='same', activation='relu')\n",
    "        \n",
    "        self.b3_1 = Conv2D(c3[0], 1, activation='relu')\n",
    "        self.b3_2 = Conv2D(c3[1], 5, padding='same', activation='relu')\n",
    "        \n",
    "        self.b4_1 = MaxPooling2D(3, 1, padding='same')\n",
    "        self.b4_2 = Conv2D(c4, 1, activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        b1 = self.b1_1(x)\n",
    "        b2 = self.b2_2(self.b2_1(x))\n",
    "        b3 = self.b3_2(self.b3_1(x))\n",
    "        b4 = self.b4_2(self.b4_1(x))\n",
    "        return tf.keras.layers.Concatenate()([b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Softmax, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2  # Import l2 regularizer\n",
    "\n",
    "def create_model_advanced(num_layers=4, filters=(32, 64, 128), dropout_rate=0.25, learning_rate=0.001, weight_decay=0.001):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters[0], kernel_size=(3, 3), padding='same', input_shape=(98, 50, 1), kernel_regularizer=l2(weight_decay))) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    for i in range(num_layers-1):\n",
    "        if i == num_layers - 2:  # Last iteration, add Inception layers\n",
    "            model.add(Inception(64, (96, 128), (16, 32), 32))\n",
    "            model.add(Inception(128, (128, 192), (32, 96), 64))\n",
    "            model.add(Inception(192, (96, 208), (16, 48), 64))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "        else:\n",
    "            model.add(Conv2D(filters[1], kernel_size=(3, 3), padding='same', kernel_regularizer=l2(weight_decay)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Conv2D(filters[2], kernel_size=(3, 3), padding='same', kernel_regularizer=l2(weight_decay))) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding = 'same'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(12))  \n",
    "    model.add(Softmax())\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b20299",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = create_model_advanced(num_layers=3, filters=(32, 64, 128), dropout_rate=0.2, weight_decay=0.001)\n",
    "\n",
    "# Train the model\n",
    "inception_model.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=25, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = inception_model.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225db87f",
   "metadata": {},
   "source": [
    "# 7.1 Bayesian optimization (Tree-structured Parzen Estimator TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp\n",
    "# fmin: Stands for \"Function Minimization\".\n",
    "# tpe: Stands for \"Tree-structured Parzen Estimator\".\n",
    "# hp: Stands for \"Hyperparameters\".\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'num_layers': hp.choice('num_layers', [2, 3, 4, 6, 8]),\n",
    "    'filters': hp.choice('filters', [(8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)]),\n",
    "\n",
    "    'dropout_rate': hp.choice('dropout_rate', [0.0, 0.1, 0.15, 0.2, 0.25]),\n",
    "    'weight_decay': hp.choice('weight_decay', [0.0, 0.1, 0.01, 0.001, 0.0001]),\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = create_model_advanced(**params)  # Create model with current parameters\n",
    "\n",
    "    # Train the model and evaluate on the test set\n",
    "    model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
    "    loss, accuracy = model.evaluate(x_val, y_val)\n",
    "    return {'loss': -accuracy, 'status': 'ok'}\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fcfe0",
   "metadata": {},
   "source": [
    "- This is the last page of the training process since it contains a huge number of pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebcc24",
   "metadata": {},
   "source": [
    " 1/37 ━━━━━━━━━━━━━━━━━━━━ 2:30 4s/step - accuracy: 0.3750 - loss: 2.1478\n",
    " 2/37 ━━━━━━━━━━━━━━━━━━━━ 7s 228ms/step - accuracy: 0.3984 - loss: 2.0991\n",
    " 3/37 ━━━━━━━━━━━━━━━━━━━━ 20s 616ms/step - accuracy: 0.4149 - loss: 2.0402\n",
    " 4/37 ━━━━━━━━━━━━━━━━━━━━ 14s 443ms/step - accuracy: 0.4206 - loss: 2.0252\n",
    " 5/37 ━━━━━━━━━━━━━━━━━━━━ 16s 500ms/step - accuracy: 0.4302 - loss: 1.9947\n",
    " 6/37 ━━━━━━━━━━━━━━━━━━━━ 13s 419ms/step - accuracy: 0.4366 - loss: 1.9628\n",
    " 7/37 ━━━━━━━━━━━━━━━━━━━━ 10s 364ms/step - accuracy: 0.4438 - loss: 1.9308\n",
    " 8/37 ━━━━━━━━━━━━━━━━━━━━ 9s 324ms/step - accuracy: 0.4488 - loss: 1.9065 \n",
    " 9/37 ━━━━━━━━━━━━━━━━━━━━ 8s 296ms/step - accuracy: 0.4530 - loss: 1.8866\n",
    "10/37 ━━━━━━━━━━━━━━━━━━━━ 7s 270ms/step - accuracy: 0.4568 - loss: 1.8709\n",
    "11/37 ━━━━━━━━━━━━━━━━━━━━ 6s 250ms/step - accuracy: 0.4602 - loss: 1.8568\n",
    "12/37 ━━━━━━━━━━━━━━━━━━━━ 5s 236ms/step - accuracy: 0.4628 - loss: 1.8435\n",
    "13/37 ━━━━━━━━━━━━━━━━━━━━ 6s 272ms/step - accuracy: 0.4648 - loss: 1.8331\n",
    "14/37 ━━━━━━━━━━━━━━━━━━━━ 5s 258ms/step - accuracy: 0.4662 - loss: 1.8264\n",
    "15/37 ━━━━━━━━━━━━━━━━━━━━ 5s 244ms/step - accuracy: 0.4679 - loss: 1.8195\n",
    "16/37 ━━━━━━━━━━━━━━━━━━━━ 4s 232ms/step - accuracy: 0.4691 - loss: 1.8143\n",
    "17/37 ━━━━━━━━━━━━━━━━━━━━ 4s 222ms/step - accuracy: 0.4699 - loss: 1.8115\n",
    "18/37 ━━━━━━━━━━━━━━━━━━━━ 4s 213ms/step - accuracy: 0.4709 - loss: 1.8077\n",
    "19/37 ━━━━━━━━━━━━━━━━━━━━ 3s 205ms/step - accuracy: 0.4722 - loss: 1.8033\n",
    "20/37 ━━━━━━━━━━━━━━━━━━━━ 3s 198ms/step - accuracy: 0.4734 - loss: 1.7994\n",
    "21/37 ━━━━━━━━━━━━━━━━━━━━ 3s 191ms/step - accuracy: 0.4746 - loss: 1.7953\n",
    "22/37 ━━━━━━━━━━━━━━━━━━━━ 3s 215ms/step - accuracy: 0.4754 - loss: 1.7924\n",
    "23/37 ━━━━━━━━━━━━━━━━━━━━ 2s 209ms/step - accuracy: 0.4764 - loss: 1.7895\n",
    "24/37 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - accuracy: 0.4774 - loss: 1.7870\n",
    "25/37 ━━━━━━━━━━━━━━━━━━━━ 2s 197ms/step - accuracy: 0.4785 - loss: 1.7841\n",
    "26/37 ━━━━━━━━━━━━━━━━━━━━ 2s 191ms/step - accuracy: 0.4796 - loss: 1.7814\n",
    "27/37 ━━━━━━━━━━━━━━━━━━━━ 1s 187ms/step - accuracy: 0.4806 - loss: 1.7790\n",
    "28/37 ━━━━━━━━━━━━━━━━━━━━ 1s 183ms/step - accuracy: 0.4813 - loss: 1.7778\n",
    "29/37 ━━━━━━━━━━━━━━━━━━━━ 1s 178ms/step - accuracy: 0.4820 - loss: 1.7764\n",
    "30/37 ━━━━━━━━━━━━━━━━━━━━ 1s 174ms/step - accuracy: 0.4829 - loss: 1.7746\n",
    "31/37 ━━━━━━━━━━━━━━━━━━━━ 1s 171ms/step - accuracy: 0.4837 - loss: 1.7730\n",
    "32/37 ━━━━━━━━━━━━━━━━━━━━ 0s 167ms/step - accuracy: 0.4843 - loss: 1.7718\n",
    "33/37 ━━━━━━━━━━━━━━━━━━━━ 0s 164ms/step - accuracy: 0.4850 - loss: 1.7705\n",
    "34/37 ━━━━━━━━━━━━━━━━━━━━ 0s 161ms/step - accuracy: 0.4856 - loss: 1.7691\n",
    "35/37 ━━━━━━━━━━━━━━━━━━━━ 0s 158ms/step - accuracy: 0.4862 - loss: 1.7678\n",
    "36/37 ━━━━━━━━━━━━━━━━━━━━ 0s 156ms/step - accuracy: 0.4868 - loss: 1.7661\n",
    "37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 264ms/step - accuracy: 0.4875 - loss: 1.7643\n",
    "37/37 ━━━━━━━━━━━━━━━━━━━━ 14s 265ms/step - accuracy: 0.4882 - loss: 1.7626\n",
    "\n",
    "100%|█████| 20/20 [3:14:46<00:00, 584.32s/trial, best loss: -0.5123825669288635]\n",
    "\n",
    "Best parameters found:\n",
    "{'dropout_rate': 2, 'filters': 3, 'num_layers': 4, 'weight_decay': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "optimized_inception_model.fit(x_train_combined1, y_train_combined1, \n",
    "                              batch_size=16, \n",
    "                              epochs=25, \n",
    "                              validation_split=0.5, \n",
    "                              callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = optimized_inception_model.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65169a79",
   "metadata": {},
   "source": [
    "- With this advanced model, the results are not as expected. I anticipated achieving higher accuracy, especially since I implemented an inception class, which includes using parallel convolutional layers. These layers are supposed to aid in extracting more features from the data, thereby increasing accuracy. However, it appears that there are other factors at play that could contribute to improving accuracy for this model, such as employing more advanced optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa6435",
   "metadata": {},
   "source": [
    "- The highest accuracy has been extracted from Bayesian_model_2 (in section 5), as it achieves a 77% accuracy in predicting true spectrograms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
